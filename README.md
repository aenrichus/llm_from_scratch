# Build an LLM from Scratch

Welcome to my personal learning project! ğŸ‰

This repository documents my journey of **building a Large Language Model (LLM) from scratch** using PyTorch. The primary goal is educational â€” to deepen my understanding of how LLMs work under the hood, from tokenization to text generation.

## What's Inside

- ğŸ§  A custom GPT-style model built with PyTorch
- ğŸ”¤ Tokenization using `tiktoken` for GPT-2 compatibility
- ğŸ“ˆ Training and evaluation loops with loss tracking and token sampling
- ğŸ” Support for top-k sampling and temperature scaling
- ğŸ§ª Loss calculation and performance metrics
- ğŸ’¾ Save/load functionality for models and optimizer states
- ğŸ› ï¸ A simple text generation interface with context input
- ğŸ§¹ A fine-tuned version of the model for spam classification

## Why I'm Doing This

I've always been fascinated by the inner workings of LLMs. Rather than relying on high-level libraries, I wanted to **go deep into the nuts and bolts** of architecture, training, and inference â€” all from first principles.

## How to Run

1. Install dependencies:
   ```bash
   pip install torch tiktoken matplotlib
   ```

2. Prepare a training text file (`decay.txt`) in the project root.

3. Run the training script:
   ```bash
   python pretraining.py
   ```

## Notes

- This project is a **learning exercise**, not intended for production use.
- Iâ€™m actively iterating on this codebase as I explore new ideas and features. This includes experimenting with downstream tasks like spam classification.
- Feedback, suggestions, and collaboration are always welcome!

## License

This project is open source and available under the MIT License.
