{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1476ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6034c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the instruction dataset\n",
    "def download_and_load_file(file_path, url):\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it to the specified file path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {url} to {file_path}...\")\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "                file.write(text_data)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"{file_path} already exists. Skipping download.\")\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aef4cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction-data.json already exists. Skipping download.\n",
      "Loaded 1100 instruction-response pairs.\n",
      "Example entry: \n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
      "Another entry: \n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(f\"Loaded {len(data)} instruction-response pairs.\")\n",
    "print(\"Example entry: \\n\", data[50])\n",
    "print(\"Another entry: \\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf85c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Alpaca prompt formatting\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "          f\"Below is an instruction that describes a task. \"\n",
    "          f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "          f\"### Instruction:\\n{entry['instruction']}\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    input_text = (f\"### Input:\\n{entry['input']}\\n\\n\" if entry['input'] else \"\")\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631afbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n",
      "\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n",
      "\n",
      "\n",
      "Training data size: 935\n",
      "Validation data size: 55\n",
      "Testing data size: 110\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"### Response:\\n{data[50]['output']}\\n\\n\"\n",
    "print(model_input + desired_response)\n",
    "\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"### Response:\\n{data[999]['output']}\\n\\n\"\n",
    "print(model_input + desired_response)\n",
    "\n",
    "# Partition the dataset for training, validation, and testing\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.10)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Testing data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement instruction dataset class\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instuction_plus_input = format_input(entry)\n",
    "            response_text = f\"### Response:\\n{entry['output']}\\n\\n\"\n",
    "            full_text = instuction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a32940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff027bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom collate function\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_len=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader.\n",
    "    \"\"\"\n",
    "    batch_max_len = max(len(item)+1 for item in batch)\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_len - len(new_item)))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_len is not None:\n",
    "            inputs = inputs[:allowed_max_len]\n",
    "            targets = targets[:allowed_max_len]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d899b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = [inputs_1, inputs_2, inputs_3]\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "689b258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668e90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    allowed_max_len=1024,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6050bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize data loaders\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_dataloader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a8b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        key = self.W_key(x)\n",
    "        query = self.W_query(x)\n",
    "        value = self.W_value(x)\n",
    "\n",
    "        key = key.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        value = value.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        query = query.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "        query = query.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(query, key.transpose(2, 3))\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = torch.matmul(attn_weights, value).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        output = self.out_proj(context_vector)\n",
    "        return output\n",
    "\n",
    "# Implementation of LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "# Implementation of GELU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "# Implementation of FeedForward layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Implement the transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.ln1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ln2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x # shortcut for attention block\n",
    "        x = self.ln1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x # shortcut for feedforward block\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "# Implementation of the full GPT model\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(* [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8d70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading OpenAI weights into our GPT model code\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
    "    return nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # Load token embedding weights\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "    \n",
    "    # Load position embedding weights\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    \n",
    "    # Load transformer block weights\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ffn.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ffn.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ffn.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ffn.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ln1.scale = assign(\n",
    "            gpt.trf_blocks[b].ln1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ln1.shift = assign(\n",
    "            gpt.trf_blocks[b].ln1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ln2.scale = assign(\n",
    "            gpt.trf_blocks[b].ln2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ln2.shift = assign(\n",
    "            gpt.trf_blocks[b].ln2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        # Load final layer norm weights\n",
    "        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1e50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configurations for the model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12\n",
    "    },\n",
    "    \"gpt2-medium (355M)\": {\n",
    "        \"emb_dim\": 1024,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 24\n",
    "    },\n",
    "    \"gpt2-large (774M)\": {\n",
    "        \"emb_dim\": 1280,\n",
    "        \"n_heads\": 20,\n",
    "        \"n_layers\": 36\n",
    "    },\n",
    "    \"gpt2-xl (1558M)\": {\n",
    "        \"emb_dim\": 1600,\n",
    "        \"n_heads\": 25,\n",
    "        \"n_layers\": 48\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964c1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 355M\n"
     ]
    }
   ],
   "source": [
    "# Choose model\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "print(f\"Model size: {model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3206a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Download and load the model\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "428aa5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights into the model\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa2dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample input\n",
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e5ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement text to token IDs and vice versa\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    decoded = tokenizer.decode(flat.tolist())\n",
    "    return decoded\n",
    "\n",
    "# Implement text generation function with temperature and top-k sampling\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            if top_k is not None:\n",
    "                top_logits, _ = torch.topk(logits, top_k)\n",
    "                min_val = top_logits[:, -1]\n",
    "                logits = torch.where(\n",
    "                    logits < min_val,\n",
    "                    torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                    logits\n",
    "                )\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            if idx_next == eos_id:\n",
    "                break\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78cba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal\n"
     ]
    }
   ],
   "source": [
    "# Test the generate function\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "# print(generated_text)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a19dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the loss calculation function\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    logits_flat = logits.flatten(0, 1)\n",
    "    targets_flat = target_batch.flatten()\n",
    "    loss = nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5436baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the training loop\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # Logging gradients for debugging\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         grad = param.grad\n",
    "            #         print(f\"{name}: grad mean={grad.mean():.6f}, std={grad.std():.6f}, max={grad.abs().max():.6f}\")\n",
    "            #         grad_norm = grad.norm().item()\n",
    "            #         print(f\"{name}: grad norm = {grad_norm:.4f}\")\n",
    "            #         if grad_norm > 1000:  # Adjust threshold as needed\n",
    "            #             print(f\"🚨 Warning: Exploding gradient in {name}!\")\n",
    "            #     else:\n",
    "            #         print(f\"{name}: grad is None\")\n",
    "\n",
    "            # add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.6)\n",
    "\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch* {epoch+1}, (Step {global_step:06d}) :\" \n",
    "                      f\"Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size,\n",
    "            eos_id=50256\n",
    "            )\n",
    "    \n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].strip()\n",
    "    print(generated_text)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35398834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send model to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de86327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.9327\n",
      "Validation loss: 3.8647\n"
     ]
    }
   ],
   "source": [
    "# Test the loss calculation function\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_dataloader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Train loss: {train_loss:.4f}\")\n",
    "print(f\"Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1958f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch* 1, (Step 000000) :Train Loss: 2.7525, Val Loss: 2.7335\n",
      "Epoch* 1, (Step 000005) :Train Loss: 1.1577, Val Loss: 1.0916\n",
      "Epoch* 1, (Step 000010) :Train Loss: 0.8406, Val Loss: 0.9223\n",
      "Epoch* 1, (Step 000015) :Train Loss: 0.8198, Val Loss: 0.8737\n",
      "Epoch* 1, (Step 000020) :Train Loss: 0.7403, Val Loss: 0.8481\n",
      "Epoch* 1, (Step 000025) :Train Loss: 0.7402, Val Loss: 0.8334\n",
      "Epoch* 1, (Step 000030) :Train Loss: 0.7706, Val Loss: 0.8057\n",
      "Epoch* 1, (Step 000035) :Train Loss: 0.6973, Val Loss: 0.7874\n",
      "Epoch* 1, (Step 000040) :Train Loss: 0.6606, Val Loss: 0.7955\n",
      "Epoch* 1, (Step 000045) :Train Loss: 0.6132, Val Loss: 0.7778\n",
      "Epoch* 1, (Step 000050) :Train Loss: 0.6476, Val Loss: 0.7674\n",
      "Epoch* 1, (Step 000055) :Train Loss: 0.7439, Val Loss: 0.7466\n",
      "Epoch* 1, (Step 000060) :Train Loss: 0.7010, Val Loss: 0.7416\n",
      "Epoch* 1, (Step 000065) :Train Loss: 0.6403, Val Loss: 0.7357\n",
      "Epoch* 1, (Step 000070) :Train Loss: 0.5184, Val Loss: 0.7257\n",
      "Epoch* 1, (Step 000075) :Train Loss: 0.5498, Val Loss: 0.7229\n",
      "Epoch* 1, (Step 000080) :Train Loss: 0.5878, Val Loss: 0.7209\n",
      "Epoch* 1, (Step 000085) :Train Loss: 0.4944, Val Loss: 0.7074\n",
      "Epoch* 1, (Step 000090) :Train Loss: 0.5480, Val Loss: 0.6859\n",
      "Epoch* 1, (Step 000095) :Train Loss: 0.4857, Val Loss: 0.6815\n",
      "Epoch* 1, (Step 000100) :Train Loss: 0.4923, Val Loss: 0.6735\n",
      "Epoch* 1, (Step 000105) :Train Loss: 0.5435, Val Loss: 0.6621\n",
      "Epoch* 1, (Step 000110) :Train Loss: 0.5331, Val Loss: 0.6578\n",
      "Epoch* 1, (Step 000115) :Train Loss: 0.4965, Val Loss: 0.6535\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "\n",
      "Epoch* 2, (Step 000120) :Train Loss: 0.4181, Val Loss: 0.6616\n",
      "Epoch* 2, (Step 000125) :Train Loss: 0.4351, Val Loss: 0.6785\n",
      "Epoch* 2, (Step 000130) :Train Loss: 0.4409, Val Loss: 0.6740\n",
      "Epoch* 2, (Step 000135) :Train Loss: 0.4005, Val Loss: 0.6770\n",
      "Epoch* 2, (Step 000140) :Train Loss: 0.4149, Val Loss: 0.6767\n",
      "Epoch* 2, (Step 000145) :Train Loss: 0.3646, Val Loss: 0.6789\n",
      "Epoch* 2, (Step 000150) :Train Loss: 0.3688, Val Loss: 0.6701\n",
      "Epoch* 2, (Step 000155) :Train Loss: 0.4154, Val Loss: 0.6701\n",
      "Epoch* 2, (Step 000160) :Train Loss: 0.4262, Val Loss: 0.6878\n",
      "Epoch* 2, (Step 000165) :Train Loss: 0.3927, Val Loss: 0.6841\n",
      "Epoch* 2, (Step 000170) :Train Loss: 0.3318, Val Loss: 0.6717\n",
      "Epoch* 2, (Step 000175) :Train Loss: 0.3634, Val Loss: 0.6657\n",
      "Epoch* 2, (Step 000180) :Train Loss: 0.4098, Val Loss: 0.6500\n",
      "Epoch* 2, (Step 000185) :Train Loss: 0.4281, Val Loss: 0.6536\n",
      "Epoch* 2, (Step 000190) :Train Loss: 0.3500, Val Loss: 0.6507\n",
      "Epoch* 2, (Step 000195) :Train Loss: 0.3517, Val Loss: 0.6420\n",
      "Epoch* 2, (Step 000200) :Train Loss: 0.3167, Val Loss: 0.6464\n",
      "Epoch* 2, (Step 000205) :Train Loss: 0.3627, Val Loss: 0.6447\n",
      "Epoch* 2, (Step 000210) :Train Loss: 0.3913, Val Loss: 0.6406\n",
      "Epoch* 2, (Step 000215) :Train Loss: 0.4063, Val Loss: 0.6375\n",
      "Epoch* 2, (Step 000220) :Train Loss: 0.3007, Val Loss: 0.6452\n",
      "Epoch* 2, (Step 000225) :Train Loss: 0.3341, Val Loss: 0.6493\n",
      "Epoch* 2, (Step 000230) :Train Loss: 0.3045, Val Loss: 0.6444\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "\n",
      "### Response:\n",
      "The meal is cooked every day by the chef.\n",
      "\n",
      "\n",
      "Execution time: 2.91 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_dataloader, val_dataloader, optimizer, device, num_epochs=num_epochs, \n",
    "    eval_freq=5, eval_iter=5, start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Execution time: {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1543aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation losses\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label='Train Loss', color='blue')\n",
    "    ax1.plot(epochs_seen, val_losses, label='Validation Loss', color='orange')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel('Tokens Seen')\n",
    "    fig.tight_layout()\n",
    "    plt.show() # Uncomment to show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b19ed974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f785e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "\n",
      "\n",
      "Correct response:\n",
      "The car is as fast as lightning.\n",
      "Generated response:\n",
      "The car is as fast as a cheetah.\n",
      "--------------------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "\n",
      "\n",
      "Correct response:\n",
      "The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "Generated response:\n",
      "A thunderstorm is a type of cloud that typically forms when thunderstorms produce thunderstorms.\n",
      "--------------------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "\n",
      "\n",
      "Correct response:\n",
      "Jane Austen.\n",
      "Generated response:\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract sample test responses\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    print(input_text)\n",
    "    \n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    \n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    print(f\"\\nCorrect response:\\n{entry['output']}\")\n",
    "    print(f\"Generated response:\\n{response_text.strip()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d21af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:32<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'generated_response': 'The car is as fast as a cheetah.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Genrate and save test set responses\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    \n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"generated_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)\n",
    "\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ca6e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gpt2-medium355M_sft.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}_sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acefa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a process is running\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c02648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running:  True\n"
     ]
    }
   ],
   "source": [
    "# Check the ollama is running\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running: \", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37eaa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query local ollama model\n",
    "def query_model(\n",
    "        prompt,\n",
    "        model=\"llama3\",\n",
    "        url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    \n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec0afd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# Test that ollama is running\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fd8412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> I'd score this response a 90 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to that of an animal (a cheetah).\n",
      "* The comparison is relevant and makes sense, as cheetahs are known for their incredible speed.\n",
      "* The language is simple and easy to understand, making it accessible to a wide range of readers.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that the comparison could be considered slightly less vivid or surprising than some other options (like \"as fast as lightning\" or \"as fast as a speeding bullet\"). However, overall, this response effectively completes the request and provides a clear and engaging comparison.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A thunderstorm is a type of cloud that typically forms when thunderstorms produce thunderstorms.\n",
      "\n",
      "Score:\n",
      ">> I'd rate this response a 20 out of 100.\n",
      "\n",
      "The response is not accurate and does not address the question being asked. The instruction asks what type of cloud is typically associated with thunderstorms, but the response describes thunderstorms as a type of cloud instead. This is a significant deviation from the original question and does not provide a relevant or correct answer.\n",
      "\n",
      "A good response would have answered the question directly by stating that cumulonimbus clouds are typically associated with thunderstorms.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> \n",
      "\n",
      "Score:\n",
      ">> I'd rate this response as 100!\n",
      "\n",
      "The instruction asks for the author of \"Pride and Prejudice\", and the correct answer is indeed Jane Austen. The response matches the instruction perfectly, providing the exact information requested.\n",
      "\n",
      "So, I'd give it a perfect score of 100!\n",
      "\n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the ollama scoring model with a few examples\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the model input `{format_input(entry)}` \"\n",
    "        f\"and the correct output `{entry['output']}`, \"\n",
    "        f\"score the response `{entry['generated_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry[\"output\"])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"generated_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n\", \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "286de025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate instruction finetuning model\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Generating scores\"):\n",
    "        prompt = (\n",
    "            f\"Given the model input `{format_input(entry)}` \"\n",
    "            f\"and the correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}` \"\n",
    "            f\"on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        # print('<<<', entry['generated_response'], '>>>\\n') # Uncomment for debugging\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Error converting score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27eb650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scores:   7%|▋         | 8/110 [00:03<01:07,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting score: I'd rate my response as follows:\n",
      "\n",
      "* For the instruction \"Generate a humorous anecdote\", I would give myself a score of 80. While my response is not a traditional joke or pun, it's a lighthearted and playful story that might elicit a chuckle.\n",
      "* For the instruction \"What is the opposite of 'happy'\"', I would give myself a score of 100. My response is a straightforward and accurate answer to the question.\n",
      "\n",
      "So, my scores are: 80 and 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating scores: 100%|██████████| 110/110 [00:27<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 109 of 110\n",
      "Average score: 61.73\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate average scores for the test set\n",
    "scores = generate_model_scores(test_data, \"generated_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores) / len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8c449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
